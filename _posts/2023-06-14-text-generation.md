---
title: 文本生成策略
date: 2023-06-14 00:00:00
author: onlyzdd
categories: [自然语言处理, 文本生成]
tags: [nlp, text generation]
math: true
---

文本生成策略，即解码策略，是指自然语言生成（Natural language generation，NLG）任务中，对当前词或词元概率分布（一般由通过 Softmax 函数产生的）进行采样的方法。通过调整文本生成策略，可以使生成的文本更加多样、更加连贯、更加和谐。

## 确定性策略

确定性策略（Deterministic strategies）是指在输入固定时，模型的输出总是固定的，这种类型的策略产生的输出缺乏丰富性、多样性。

### 贪婪搜索

贪婪搜索（Greedy search）指每次都选择概率最高的词作为结果，因此最终的结果序列是局部最优，而非全局最优。

使用贪婪搜索，对于同一输入，输出总是一致的，所以缺乏丰富性。此外，对于长输出，贪婪搜索的结果往往会产生重复的片段。

### 柱搜索

为了解决贪婪搜索的问题，柱搜索（Beam search，也叫束搜索）通过设置柱的数目 $N$ 和柱的宽度 $K$，在每次选择时，保留前 $N$ 步中概率最高的 $K$ 个词，通过计算概率值之积，进而选取最优结果。也可以生成多个候选，以增加结果的多样性。

相比于贪婪搜索，可以获得更好的局部最优，但增加了不少运算量。

## 随机性策略

相比于确定性策略，随机性策略（Stochastic strategies）可以使输出多样化，即根据词的概率值进行随机采样。

然而，直接对所有词的概率值进行随机采样容易产生风险，一个比较好的方法是控制词的候选集合，在这些词的集合上，对概率值进行重新归一化、采样。随机性策略往往可以与柱搜索结合，以产生更好的结果。

### Top-K 搜索

Top-K 搜索是指选择概率值最大的 $K$ 个词作为候选集，然后进行归一化、采样。一般的步骤为：

1. 对词表进行排序，选择概率最高的 $K$ 个词构成候选集
2. 对候选集中的概率值重新进行归一化，构成新的概率分布
3. 从新的概率分布中随机采样一个词

一般，候选集大小的取值范围为 $K < 100$。

### Top-p 搜索

Top-p 搜索（又称 Nucleus 采样）是指根据概率对词进行排序，当候选集中所有词的概率和小于预设值 $p$ 时向其中不断加入概率最大的词。最后对候选集进行归一化、采样，一般的步骤为：

1. 初始化候选集为空，设置预设值 $p$
2. 根据概率对词表进行排序，不断向候选集加入概率最高的词，超过预设值时则停止
3. 对候选集中的概率值重新进行归一化，构成新的概率分布
4. 从新的概率分布中随机采样一个词

一般，概率和预设值的取值范围为 $p\in [0.9, 1)$

### 对比搜索

对比搜索（Contrastive search）基于 TopK 搜索，其在产生输出时将综合考虑当前输出的概率以维持输出的连贯性、当前输出与已有上下文之间的相似性以降低重复输出的可能性，即：

$$
x_t = \underset{v \in V^{(k)}}{\arg max}
	  \{ 
	  	(1 - \alpha) \times 
	  	\underbrace{p_\theta(v|\boldsymbol{x}_{<t})}_{\mathrm{model\ confidence}}
	  	- \alpha \times
	  	\underbrace{\max\{ s(h_v, h_{x_j}: 1\leq j\leq t- 1) \}}_{\rm{degeneration\ penaly}}
	  \}
$$

其中 $V^{(k)}$ 是语言模型概率输出中概率最大的 $k$ 个词，该公式的第一项为模型置信度，即词的概率值；第二项为惩罚项，计算当前词与上下文的余弦相似度，进行惩罚，其中 $h_{x_j}$ 为上下文 $\boldsymbol{x}_j$ 的表示。

对比搜索的可调参数为候选词数 $K$ 和惩罚系数 $\alpha$，当 $\alpha=0$ 时，对比搜索退化为贪婪搜索。

## 参数

### 温度

文本生成模型的输出一般是各个词的 logits，需要在其上使用 Softmax 进行标准化，以得到各个词的概率值，即

$$
\cfrac{\exp(z_i)}{\sum_j \exp(z_j)}
$$

而在随机采样中，直接对上述概率值进行采样可能会得到不合理的结果。为了解决这个问题，可以增加温度参数 $T > 0$，来控制概率值分布的尖锐程度。这被称为 Softmax with Temperature，在改变概率分布的同时保持概率之间的相对大小不变，即

$$
\frac{\exp(z_i / T)}{\sum_j\exp(z_j / T)}
$$

对于参数 $T$：

1. $T$ 越小，概率值分布越尖锐，当 $T \rightarrow 0$ 时，相当于确定性策略中的贪婪搜索
2. $T$ 越大，概率值分布越平缓，当 $T \rightarrow \infty$ 时，相当于从均匀分布中随机采样

温度参数一般与 Top-K、Top-p 采样策略一起使用，在不影响文本质量的情况下，可以适当提高 $T$，以提升模型用词的丰富性。

### 其他参数

为使文本生成模型减少重复输出并提升输出的连贯性，通常可以设置以下参数：

1. 输出的最大/最小长度：强制达到指定长度后停止生成或达到指定长度前禁止结束词
2. N-gram 惩罚：防止某些组合出现次数过多，但不利于一些与主题相关的名词短语
2. 强制/禁止词的出现：用于强制输出特定词，或禁止有害词出现

## 参考链接

1. [Huggingface Blog: How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)
1. [阿里云：ICBU 可控文本生成技术详解](https://zhuanlan.zhihu.com/p/415309828)

